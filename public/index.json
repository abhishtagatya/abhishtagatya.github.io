{"categories":[{"link":"/categories/experiment/","name":"Experiment","slug":"Experiment"},{"link":"/categories/game/","name":"Game","slug":"Game"},{"link":"/categories/game-assets/","name":"Game Assets","slug":"Game Assets"},{"link":"/categories/game-mechanics/","name":"Game Mechanics","slug":"Game Mechanics"},{"link":"/categories/tool/","name":"Tool","slug":"Tool"}],"pages":[],"posts":[{"link":"/posts/building-a-keyboard-mouse-sprite-animation-library/","text":" TL;DR This started as a small project to create a few sprites and a simple system for my game Quality Assurance to handle sprite animations for CTAs. It grew into a fully modular animation library for Unity.\nThe library is lightweight and fast, packed with 13+ base animations (Pulse, Slide, Node, Fade, etc.). All animations are modular and can be combined to create more complex effects. You can also script your own animations and run them through the library for easy management.\nIntroduction Recently, I started working on a First-Person Psychological Horror game in Unity. It required a system to handle keyboard and mouse inputs for object interactions. Some of these inputs needed players’ attention, along with CTA (Call-to-Action) animations and visual guidance for complex actions (e.g., shaking the mouse, moving it left/right, or in any chosen direction).\nInstead of building only the bare minimum, I created a sprite animation library aimed at helping solo developers avoid reinventing the wheel. Many existing systems are feature-rich but bloated and expensive, often offering more than what’s really needed.\nI designed this library to be modular: animations can harmonize, overlap, or even cancel each other. This flexibility makes it easier to visually communicate with players.\nDevelopment I began by researching UI animation approaches. The key questions were:\nWhat are the trade-offs between scripted animations and Unity’s Animator? How should the animations be packaged? How can level designers use this system without friction? Answering these questions was crucial before building something useful for others, not just myself.\nPerformance Trade-Offs Performance is critical for UI. Animations should have minimal overhead. In Unity, you can animate using either:\nScripts (update Transform, CanvasGroup, or other components programmatically using math like interpolation and wave functions). Animators (keyframes and animation clips). Unity’s Animator is ideal for non-technical designers—it’s intuitive and quick for creating simple to complex animations. But after researching, I found scripted animations to be better for performance and flexibility.\nHere’s a quick comparison:\nOption Pros Cons Best Use Case Script Lightweight, no allocations, customizable, parameterized, tweakable at runtime Harder to manage for complex animations, more code to maintain Small/simple animations (shakes, pulses, color changes), performance-critical scenarios Animator Visual editor, easy for artists, reusable clips, good for complex sequences More overhead, harder to parameterize runtime behavior, asset-heavy Multi-step animations, artist-driven workflows, complex UI sequences Tween Libraries (DOTween, LeanTween, etc.) Concise code, easing, sequences, callbacks, supports runtime param changes Small allocations, external dependency, slight performance cost Prototyping, quick setup, readable code, convenience over micro-optimization I opted for a scripted approach as the default, with an optional hybrid system that lets you plug in your own Animation Clips if needed. This provides modularity while keeping performance tight.\nPackaging the Animation Using scripts, I built a set of common animations (Pop, Pulse, Wiggle, etc.) frequently used for CTAs and highlights. One component, KBMAnimation, exposes all built-in animations, letting users select, configure, and tweak parameters easily.\nKBMAnimation Component Selection Each animation shares common properties:\nDuration Looping behavior Delay between loops Timing (linear vs curve-based) These are exposed in a simple, intuitive way for designers.\nKBMAnimation Animation Settings For extra visual flair, I added particle trails to emphasize motion. Unity’s built-in Trail Renderer wasn’t suited for UI (it’s designed for World Space), so I implemented a custom lightweight Trail system using object pooling to spawn sprites for ghost or dotted trail effects.\nSince multiple KBMAnimation components can run on the same object, I also added a way to define relationships between the base animation and its trail.\nKBMAnimation Trail Settings This modular structure, combined with customizable curves, creates an elegant foundation for building and extending animations. Power users can easily add their own effects or adapt the system for new use cases.\nC# Collapse Copy 1void SomeAnimation() 2{ 3 if (elapsedTime \u0026lt; duration) 4 { 5 // Interpolate over Time, evaluated with the Time Animation Curve 6 float t = elapsedTime / duration; 7 InterpolateAndApply(x, y, timeCurve.Evaluate(t)); 8 9 // Spawn Trail (if set) 10 if (animationTrail != null) animationTrail.SpawnTrail(); 11 12 // Update elapsed time 13 elapsedTime += Time.deltaTime; 14 } 15 else 16 { 17 // Reset state 18 SomeResetFunction(); 19 20 // Try looping 21 TryLoop(); 22 } 23} Click to expand and view more Building Designer Experience The usability of this library is key to its success. Designer/Developer Experience (DX) was a top priority, so I created Editor utilities to minimize documentation overhead.\nSample Scene: Includes common use cases. Designers can copy prefabs directly into their game scenes for instant plug-and-play.\nResponsive Editor UI: Since the component is heavily parameterized, unused options are hidden based on the selected animation. This keeps the interface clean and intuitive.\nTooling and Docs: Tooltips, warnings, and inline documentation help designers understand behavior quickly. Full documentation is also included for deeper learning.\nOverall The system, GenericKBM, is scheduled for release in mid-September on the Unity Asset Store and Itch. There will be multiple options available—ranging from just the sprite set to the full animation library package.\n","title":"Building a Keyboard \u0026 Mouse Sprite Animation Library"},{"link":"/posts/building-a-custom-color-tag-encoding-pass-in-unity-urp/","text":" Introduction In my game Quality Assurance, I needed a system to tag objects with colors and later sample that information for gameplay mechanics. Crucially, this information needed to be hidden from the player, so it must be rendered off-screen.\nThis article describes how I implemented a custom color tag encoding pass in Unity URP, using ScriptableRendererFeature and MaterialPropertyBlock.\nDevelopment Setting Up Per-Category Tags To encode tags efficiently, I use MaterialPropertyBlock to assign a color representing the object\u0026rsquo;s category tag:\nCSHARP Collapse Copy private void ApplyEncoding(Color tagColor) { MaterialPropertyBlock block = new MaterialPropertyBlock(); block.SetColor(\u0026#34;_TagColor\u0026#34;, tagColor); foreach (MeshRenderer renderer in meshRenderers) { if (renderer != null) renderer.SetPropertyBlock(block); } } Click to expand and view more Here, each object in a category shares the same tag color. This ensures objects in the same category are encoded consistently during the render pass.\nRendering Tags Off-Screen To sample tags later, we need to render only the tag information to a render target. This requires:\nA Custom Unlit Shader that outputs only the tag color. A Render Feature that overrides the object\u0026rsquo;s material with this tag material. Here’s a simple shader for the tag material:\nHLSL Collapse Copy Shader \u0026#34;Unlit/TagColor\u0026#34; { Properties { _TagColor (\u0026#34;Tag Color\u0026#34;, Color) = (0,0,0,1) } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Geometry\u0026#34; } LOD 100 Pass { Name \u0026#34;TagPass\u0026#34; tags { \u0026#34;LightMode\u0026#34;=\u0026#34;SRPDefaultUnlit\u0026#34; } ZWrite On Cull Off Blend Off HLSLPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; CBUFFER_START(UnityPerMaterial) half4 _TagColor; CBUFFER_END struct Attributes { float4 positionOS : POSITION; }; struct Varyings { float4 positionHCS : SV_POSITION; }; Varyings vert(Attributes IN) { Varyings OUT; OUT.positionHCS = TransformObjectToHClip(IN.positionOS.xyz); return OUT; } half4 frag(Varyings IN) : SV_Target { return _TagColor; } ENDHLSL } } } Click to expand and view more The Render Feature Next, we create a ScriptableRendererFeature that renders all tagged objects using the tag material:\nC# Collapse Copy public override void RecordRenderGraph(RenderGraph renderGraph, ContextContainer frameData) { if (tagMaterials == null) return; var cameraData = frameData.Get\u0026lt;UniversalCameraData\u0026gt;(); var resourceData = frameData.Get\u0026lt;UniversalResourceData\u0026gt;(); var renderingData = frameData.Get\u0026lt;UniversalRenderingData\u0026gt;(); var shaderTags = new[] { new ShaderTagId(\u0026#34;UniversalForward\u0026#34;), new ShaderTagId(\u0026#34;SRPDefaultUnlit\u0026#34;) }; var desc = new RendererListDesc(shaderTags, renderingData.cullResults, cameraData.camera) { rendererConfiguration = PerObjectData.None, renderQueueRange = RenderQueueRange.opaque, sortingCriteria = SortingCriteria.CommonOpaque, overrideMaterial = tagMaterials[ctType] }; using (var builder = renderGraph.AddRasterRenderPass\u0026lt;PassData\u0026gt;(\u0026#34;Color Tag Pass\u0026#34;, out var passData)) { passData.rendererList = renderGraph.CreateRendererList(desc); builder.UseRendererList(passData.rendererList); builder.SetRenderAttachment(resourceData.activeColorTexture, 0, AccessFlags.Write); builder.SetRenderFunc((PassData data, RasterGraphContext ctx) =\u0026gt; { ctx.cmd.DrawRendererList(data.rendererList); }); } } Click to expand and view more Key points:\nObjects are rendered off-screen using the tag material. Each object writes its category or object tag as a color. Later shaders or scripts can sample this render target for gameplay mechanics like selection, masking, or highlighting.\nUsing the Render Target Once rendered, the tag buffer can be used:\nTo determine object categories for gameplay logic. To create masks for post-processing effects. To handle dynamic interactions without exposing information to players. For instance, you can render it just once per frame to a RenderTexture and immediately disable the pass to avoid showing it in-game.\nLimitations Some caveats with this technique:\nsRGB Color Noise Colors like pure RED (1,0,0) may be interpreted differently in sRGB (0.8,0,0). This limits the number of unique categories you can safely encode. Category Limit (Caused by sRGB Color Noise) With 7 colors (RED, BLUE, GREEN, YELLOW, MAGENTA, CYAN, WHITE), you can encode 7 categories. You can combine colors for more categories, but this increases complexity and error risk. Skybox Interference Skyboxes may introduce unwanted pixels in the tag render. Workarounds include using a custom opaque skybox or enclosing the level in a sphere. Conclusion This system provides a simple, flexible way to encode per-object tags in Unity URP.\nWhile it has limitations in color precision and category count, it’s easy to implement and allows off-screen metadata rendering for gameplay mechanics like selection, masking, and procedural effects.\n","title":"Building a Custom Color Tag Encoding Pass in Unity URP"},{"link":"/posts/beta-project-quality-assurance/","text":" Quality Assurance (Beta Snapshot) TL;DR Quality Assurance is a solo game development project under the Sekochi Games production. It\u0026rsquo;s main game design is inspired by Exit 8 with a Black Mirror-like story telling. In this game, you are transported into the meta-verse to be a Quality Assurance and make sure every asset is working correctly by inspecting its looks, function, sound, and placement. The game suddenly takes a steep turn depending on the ending you choose.\nIntroduction There has been a surge of games with the theme Anomalies within the market that is popularized by streamers. The game\u0026rsquo;s premise is to inspect a generic environment filled with seemingly normal objects. Amongst them there were a couple objects that deviate from the normal, we call these as Anomalies.\nYou usually start with a room that is free from these Anomalies. Within this room, you need to memorize the layout, the looks and feels of the object around you, and if you\u0026rsquo;re confident that you\u0026rsquo;ve remembered everything, you move on to the next room with identical objects but may be spreaded with Anomalies amongst them.\nThese Anomalies may look different than the objects you\u0026rsquo;ve seen before. Usually it follows a horror them with the addition of blood, gore, or any harmful intents towards the player to build an eerie environment that keeps you on your toes.\nFollowing these game design, we tend to see a revolving copycats around the original concept of simple Spot the Difference Puzzle , with some adding rather interesting mechanics to spot the difference or adding a narrative elements to the base game itself. Other than thematic changes, the genre grew in popularity due to its simplicity but also heading towards staleness as cash-grabby developers capitalize in this trend with the lowest effort of thematic changes.\nWhat\u0026rsquo;s Special about this Game The game takes a new approach of how to report anomalies and deal with anomalies, have some narrative elements and decision making towards how you approach the game, and a game loop that is easily expandable.\nImproving the Base Game The base game of this genre usually lets player spot the anomalies, if they do find the anomalies, they need to take certain actions, usually going to a different route stating that this room has anomalies, or moving forward stating that this room is free from anomalies. The game ends when you guessed right $x$ amount in a row.\nSome variations include reporting the anomalies, where you have a special tool to report, remove, or interact with the anomalies. Either way, the premise is Spot the Difference X Times, Win the Game.\nQuality Assurance Computer Anomaly (Beta Snapshot) Quality Assurance is a game where an individual gets sucked into a meta-verse for a job that they applied for. That job is called a Quality Assurance, where they need to inspect the environment for objects that deviate. The deviation can be either Looks (the look different than before), Function (they don\u0026rsquo;t function as intended), Sound (objects that make sounds suddenly don\u0026rsquo;t or make different sound than usual), or Placement (objects is not placed in the same spot as before).\nQuality Assurance Texture Anomaly (Beta Snapshot) As a Quality Assurance, you need to not only spot these anomalies, but also report it by tagging what kind of anomaly it is. The player is given a printer camera to shoot the scene with anomalies and tag the printed picture with a special label to be evaluated. Once all of the anomalies have been reported within the level, you can move on successfully. However, doing it prematurely might make you restart to the Base Level (the level without anomalies) to relearn the structure.\nWhat Will Make it not Stale Quality Assurance is a game about Spotting and Reporting Anomalies. It immerses you into a close-to real-world experience of this profession. The game teaches you how to spot and understand the difference between by each stage you go through with seamingly increasing difficulty.\nThe game loop itself is aimed towards replayability, thus requiring development to create a procedural selection of objects to anomalize to randomize the selection, unlike other games in this genre where a certain number of anomalies is defined beforehand. With this (close to) truly random level with procedural selection for balancing, we can make endless variations of the level for the players to play in.\nOn top of this, we can create increasing challenges such as Speedrunning, Highscoring, etc. This creates an ecosystem for the existence of replayability towards achieving more in the game. With this main mechanic set into place, we can add supporting mechanics to de-linearize the game further.\nTargetting a Bigger Audience The main success of this genre was due to streamers ability to put fire on the marketing gasoline of this game. The game was simple to play while the anomalies we\u0026rsquo;re well-crafted to invoke reactions of the streamers and audience. The more reactive you can get the streamers to react, the more streamers would favor to play this game, the more recognition you get to general players who would buy your games to try out for themselves.\nTherefore, to strategize for this, Quality Assurance splits the Game Loop with the Narrative Loop of the game. This means that the game has two modes: Story Mode (Narrative Loops) takes a piece of the game loop as a story driver and a medium of how players can interact with the story in general, and Endless Mode (Pure Game Loop) puts the whole game loop to the game to achieve something replayable within it.\nThis strategy plans to make streamers play the game in Story Mode, showcasing the games mechanic proportionally, adding intensive anomalies that are more creative and shock-provoking than the usual, while also funneling the marketing into the general players. The general players tend to expect more from the game they bought, playing the Story Mode, testing everything out, playing the Endless Mode and hopefully captivates some players in the process to stay.\nOverall (Current Condition) The game is currently open for Closed Beta, many aspects of the game are being thoroughly tested for their experience, technical aspects, etc.\n","title":"(Beta) Project: Quality Assurance"},{"link":"/posts/building-a-solo-dungeon-runner-boardgame/","text":" Dungeon Run Public Playtesting Introduction With Dungeon Run, we set out to design a game that fills a unique space in the RPG genre: a short-form, solo dungeon-crawling board game with strategic depth and high replayability. While most RPG-inspired board games tend to be sprawling, multiplayer experiences that require hours to play, Dungeon Run was conceived as a 30–60 minute adventure, built for players who love to explore, experiment, and overcome challenges on their own.\nTargeted at explorers and achievers, the game blends dungeon-crawling exploration with card-based combat and deck-building mechanics, offering a layered yet approachable experience.\nHigh Concept The game takes place in the depths of the Trigonal Abyss, a mystical dungeon forged by the legendary wizard Pascala. At its core lies the Tetra Head, a reality-bending prism that leaks chaotic energy into the dungeon, reshaping its layout and spawning otherworldly monsters.\nThe player, cast as a lone adventurer, must battle through enemies, collect loot, and strengthen their deck on the way to face the dungeon’s final guardian: the Three-Headed Hydra. Victory means claiming the Tetra Head—and the power to reshape reality itself.\nOur guiding design choice was simple: explore the dungeon, defeat enemies, collect loot, upgrade your deck, and face the boss.\nDesign Choices Dungeon Run Combat Preview Combat: Controlled Randomness We wanted combat to feel both strategic and unpredictable. The solution was our 7-card enemy system:\nEach enemy has six fixed action cards and one “wild” card drawn from a special deck. This creates recognizable patterns while preserving enough randomness to keep battles fresh. Encounters scale in difficulty across three phases, introducing tougher enemies and multi-enemy battles. The player fights using a 5-card hand, playing two cards per turn before drawing new ones. Deck upgrades are limited in number, pushing players to make deliberate strategic choices.\nThis system balances predictability (learning enemy patterns) with surprise (random action draws), giving combat a tactical, evolving feel.\nDungeon Run Board Preview Exploration: The Triangular Dungeon To mirror the chaos of Pascala’s dungeon, we designed an exploration system built around triangular dungeon tiles. These tiles are laid face-down, then gradually revealed as the player chooses their path. Each tile offers different functions:\nBattle tiles trigger encounters. Forge tiles allow deck upgrades. Healing tiles restore health. Observation tiles reveal adjacent tiles, giving a glimpse into the unknown. This mechanic creates a sense of discovery and strategy. Each playthrough offers a unique dungeon layout, with branching choices that affect the player’s path to the Hydra.\nNarrative: Lore in the Mechanics Rather than presenting story through text alone, we integrated narrative directly into the rules, tiles, and encounters. For example:\nThe ruleset introduces Pascala and the dungeon’s chaotic energy. Dungeon tiles visually expand the world and hint at lore. Enemies embody fragments of Pascala’s creations. By weaving narrative into mechanics, we created a quirky, mystical dungeon crawler where the lore naturally emerges from gameplay.\nWhy This Game? Most RPG board games are expansive, multiplayer, and time-consuming. We took inspiration from those systems but intentionally designed for:\nAccessibility – shorter sessions and easier onboarding. Single-player focus – tapping into a rarely explored niche in board games. Replayability – randomized dungeons, diverse enemies, and modular card systems ensure no two runs feel the same. Key Features We’re Proud Of Unique solo experience – a board game fully designed for one player. Dynamic dungeon layout – triangular tiles create fresh layouts and strategies each game. Semi-random enemy AI – enemies with patterns you can learn, but never fully predict. Card-driven mechanics – all actions, both player and enemy, are governed by cards for tactical consistency. Strategic progression – players balance buffs, debuffs, and deck upgrades across three escalating phases. High replayability – modular systems ensure every run feels different. Conclusion Dungeon Run is our answer to the demand for immersive, strategy-driven RPGs that don’t require hours to play or multiple players to enjoy. By combining deck-building combat, tile-based exploration, and a narrative woven into mechanics, we’ve built a quirky, mystical dungeon crawler that rewards both strategic planning and curiosity.\nIt’s short, it’s sharp, and it’s designed to challenge solo adventurers who dare to descend into the Trigonal Abyss.\n","title":"Building a Solo Dungeon Runner Boardgame"},{"link":"/posts/realistic-wave-simulation-in-unity-urp/","text":" Introduction This project is a custom water shader written in HLSL for the Universal Render Pipeline (URP). It simulates dynamic waves with configurable functions, domain warping, and physically-inspired lighting, enabling both stylized and semi-realistic water rendering.\nCore Features Wave Simulation Supports sine and exponent-based wave functions. Multiple overlapping waves can be combined with adjustable frequency, speed, amplitude, and wave count. Domain warping adds natural randomness and complexity to wave motion. Normal Reconstruction Normals are recalculated using central differences for more accurate lighting. Produces realistic reflections and specular highlights that respond to wave motion. Lighting and Reflections Configurable between PBR lighting and Blinn-Phong. Supports environment reflections via a cubemap. Adjustable smoothness, metallic, and specular controls for fine-tuning water appearance. Customization Parameters Wave Settings: Count, frequency, speed, amplitude. Warp Settings: Frequency, amplitude, speed. Material Settings: Base color, metallic, smoothness, occlusion, specular highlights, and reflection intensity. Conclusion The shader demonstrates how to blend procedural wave displacement with reflection-based lighting to simulate believable water surfaces in Unity. By layering domain-warped waves and combining multiple lighting models, it balances flexibility, realism, and artistic control for interactive environments.\n","title":"Realistic Wave Simulation in Unity URP"},{"link":"/posts/visualizing-brno-in-unity/","text":" Introduction During my time at HCILab, under the supervision of Jonas Rosecký, I developed GISMU, a Unity-based system for visualizing geospatial data in 3D. The project focused on Brno, CZ, exploring how terrain, transport, and infrastructure data could be transformed into an interactive environment — with future potential for VR exploration.\nWhat We Built GISMU reconstructs Brno’s terrain from the 2019 Digital Terrain Model and layers it with multiple datasets from data.brno.cz , including:\nPublic transport lines (bus routes) Sidewalk networks Trees Streetlights The tool parses shapefiles directly (Polygon, LineString, and Point geometries) using NetTopologySuite, enabling a modular pipeline for extending to new datasets.\nChallenges \u0026amp; Design Decisions Working with millions of spatial objects in Unity was a significant challenge. To make the system performant, I implemented several optimizations:\n1. Efficient Rendering with Mesh Instancing Rendering thousands of trees, lights, and other points individually quickly overwhelmed Unity. To solve this, I used GPU mesh instancing, batching objects into groups of 1000. This reduced draw calls drastically and kept framerates stable.\n2. Raycasting for Terrain Alignment All objects from sidewalk lines, tree points, and streetlight points had to “sit” on top of the terrain mesh. Instead of guessing heights from raw data, I used raycasting against the reconstructed terrain so each object was properly aligned, even with elevation changes.\nThe raycasting is done during runtime sequentially in batched to reduce performance overhead during setup calculation.\n3. Shapefile Parsing with NetTopologySuite Using NetTopologySuite with Esri shapefile support, I was able to directly read geospatial geometries and convert them into Unity meshes. This required building a robust coordinate transformation system (SJTSK → WGS84 → Unity world coordinates) with ProjNet.\n4. Terrain Chunking for Performance The terrain of Brno covers a large area. To avoid hitting Unity’s 65k vertex per mesh limit and to improve culling, I split the terrain into uniform centroid grid chunks. Each chunk could then be managed independently, improving both memory use and runtime efficiency.\nWhy It Matters GISMU shows how game engines like Unity can be applied beyond games, giving geospatial data an interactive and immersive dimension. With potential VR integration, GISMU opens possibilities for:\nUrban planning \u0026amp; infrastructure visualization Educational tools for geography and city studies General-purpose geospatial exploration Conclusion This project combined computer graphics techniques, GIS processing, and Unity optimizations into a cohesive pipeline. The end result: a tool that turns raw shapefiles into an explorable 3D city landscape — bridging geospatial science and immersive visualization.\nYou can find the presentation and project below.\nGISMU Presentation GISMU ","title":"Visualizing Brno in Unity"},{"link":"/posts/making-procedural-clouds-using-stacked-perlin-in-glsl/","text":" Preview (https://www.shadertoy.com/view/W3BGRm ) Introduction This shader creates a dynamic sky with drifting clouds using Perlin noise and layered blending techniques. Instead of relying on bitmap textures, it generates everything procedurally, making it scalable and resolution-independent.\nHow It Works 1. Base Colors Sky: a soft blue gradient (vec3(0.4, 0.70, 0.9)). Clouds: pure white (vec3(1.0)). 2. Noise Functions hash(vec2 p) generates pseudo-random gradients. perlin(vec2 p) implements smooth Perlin noise interpolation. These form the foundation of the cloud shapes. 3. Layered Perlin Noise stackedWavedPerlin stacks 8 octaves of Perlin noise:\nEach octave doubles frequency and halves amplitude (fBm). Offset samples (+0.1, -0.1) create a wave-like drift. Animation is driven by iTime, simulating cloud movement. 4. Sky Gradient and Blending The shader mixes noise with a vertical blue gradient:\nsmoothstep layers soften cloud edges. Multiple blends simulate light scattering inside clouds. Results in natural-looking, soft formations. 5. Final Composition The sky texture is sampled at different scales (uv * 4.0, uv * 2.0), then combined to add extra detail.\nConclusion Procedural: No image textures needed. Dynamic: Clouds move and evolve over time. Stylized Realism: Smooth, layered blending mimics atmospheric scattering. This approach is widely used in games, simulations, and generative art to produce skies that feel alive without heavy assets.\nGLSL Collapse Copy vec3 SKY_COLOR = vec3(0.4, 0.70, 0.9); vec3 CLOUDS_COLOR = vec3(1.0); vec2 hash(vec2 p) { p = vec2(dot(p, vec2(127.1, 311.7)), dot(p, vec2(269.5, 183.3))); return -1.0 + 2.0 * fract(sin(p) * 43758.5453123); } float perlin(vec2 p) { vec2 i = floor(p); vec2 f = fract(p); vec2 u = f * f * (3.0 - 2.0 * f); return mix(mix(dot(hash(i + vec2(0.0, 0.0)), f - vec2(0.0, 0.0)), dot(hash(i + vec2(1.0, 0.0)), f - vec2(1.0, 0.0)), u.x), mix(dot(hash(i + vec2(0.0, 1.0)), f - vec2(0.0, 1.0)), dot(hash(i + vec2(1.0, 1.0)), f - vec2(1.0, 1.0)), u.x), u.y); } float stackedWavedPerlin(vec2 uv) { float n = 0.0; float amp = 1.0; float freq = 1.0; for (int i = 0; i \u0026lt; 8; i++) { n += perlin(uv * freq + iTime * freq * 0.1) * amp; n += perlin(uv + 0.1 * freq + iTime * freq * 0.1) * amp * 0.25; n += perlin(uv - 0.1 * freq + iTime * freq * 0.1) * amp * 0.25; freq *= 2.0; amp *= 0.5; } return n; } // Please let me know how I can improve the `skyTexture` vec3 skyTexture(vec2 uv) { float m = stackedWavedPerlin(uv); vec3 gradientSky = mix(SKY_COLOR * 0.95, SKY_COLOR, uv.y); vec3 c1 = mix(gradientSky, CLOUDS_COLOR, smoothstep(0.0, 0.5, m * 2.5)); vec3 c2 = mix(c1, CLOUDS_COLOR / 1.05, smoothstep(0.0, 1.25, m * 2.)); vec3 c3 = mix(c2, CLOUDS_COLOR / 1.2, smoothstep(0.0, 1.5, m * 1.5)); vec3 c4 = mix(c3, CLOUDS_COLOR / 1.75, smoothstep(0.0, 2.5, m)); return c4; } void mainImage(out vec4 fragColor, in vec2 fragCoord) { vec2 uv = fragCoord / iResolution.y; vec3 t = skyTexture(uv * 4.); t += skyTexture(uv * 2.) * 0.1; fragColor = vec4(t, 1.); } Click to expand and view more ","title":"Making Procedural Clouds using Stacked Perlin in GLSL"},{"link":"/posts/making-batik-in-glsl/","text":" Preview (https://www.shadertoy.com/view/3csXW4 ) Introduction This project explores the generative expression of Batik, a traditional Indonesian textile art, through GLSL shaders. By using noise functions, trigonometric patterns, and domain warping, the shader recreates floral and geometric motifs that resemble Batik designs in a dynamic, procedural form.\nTechnical Approach Base Functions\nA rotation matrix (rmatrix) and hash-based noise (hash12) provide randomized yet structured variations in the patterns. Fractal Brownian Motion (fbm) adds roughness and organic texture to avoid flat, uniform surfaces. The movement paired with the noise creates the illusion of rough texture similarly found in Cloth. Motifs and Patterns\npattern() generates repeating sinusoidal motifs, forming floral-like tessellations. trigFlower() uses polar coordinates and trigonometric modulation to create radial flower-like forms, echoing Batik’s botanical elements. Batik Floral pattern mimics the style of Sekar Jagad , but less abstracted. Color Palettes\nbatikBgTex() blends deep indigos, warm browns, and creamy yellows—colors historically used in Batik dyeing. batikMaskTex() introduces contrasting highlights, reminiscent of wax-resist dyeing, where wax masks create sharp, layered contrasts. Dynamic Motion\nSubtle rotation (rmatrix) and animated offsets ensure that the patterns feel alive, much like the rhythmic flow of Batik-making as wax and dye interact over cloth. Trying to visualize the relaxing and repetitive nature of creating Batik but also capturing the intricacy. Cultural Significance Batik is not only an art form but also a cultural heritage recognized by UNESCO. By translating Batik patterns into real-time shader code, this project shows how computational art can act as a new medium of cultural expression.\nUnlike static fabric, the shader captures Batik’s spirit in a living, digital canvas—patterns morph, colors shift, and textures evolve over time. This generative approach pays homage to Batik’s philosophy: balance between structure and improvisation, symmetry and imperfection.\nConclusion This shader is more than a technical exercise—it’s an experiment in preserving and reimagining traditional art forms in the context of digital media. By embedding Batik’s motifs into GLSL, it bridges the past and future, celebrating cultural identity through computational creativity.\nGLSL Collapse Copy mat2 rmatrix(float a) { float c = cos(a); float s = sin(a); return mat2(c, -s, s, c); } float hash12(vec2 uv) { return fract(sin(dot(uv, vec2(12.9898, 78.233))) * 43758.5453); } float fbm(vec2 uv) { float value = 0.0; float scale = 0.5; for (int i = 0; i \u0026lt; 5; i++) { value += scale * hash12(uv); uv *= 2.0; scale *= 0.5; } return value; } float pattern(vec2 uv, float sz, float pts) { uv *= sz; uv += vec2(sin(uv.y * 3.0 + iTime * 0.4) * 0.3, cos(uv.x * 3.0 + iTime * 0.4) * 0.3); float petal = sin(uv.x * pts) * cos(uv.y * pts); return smoothstep(0.2, 0.6, petal); } float trigFlower(vec2 uv, float petals) { return 1.0 - smoothstep(cos(atan(uv.y, uv.x) * petals) * 0.1 + 0.25, cos(atan(uv.y, uv.x) * petals) * 0.1 + 0.25 + 0.05, length(uv)); } vec3 batikBgTex(vec2 uv, float cn) { vec3 colors[4] = vec3[4]( vec3(0.011, 0.019, 0.062), vec3(0.847, 0.556, 0.188), vec3(0.996, 1.0, 0.780), vec3(0.752, 0.376, 0.054) ); float pt6 = pattern(uv, 8.0, 6.0); // Base Pattern float pt5 = pattern(uv, 4.0, 5.0); // Floral Pattern float pt52 = pattern(uv, 4.1, 5.0); vec3 col = mix(colors[0], colors[1], pt6 + cn * 0.5); if (pt52 \u0026gt;= 0.99) { col = mix(colors[0], colors[3], pt52 + cn * 0.2); } if (pt5 \u0026gt;= 0.99) { col = mix(colors[0], colors[2], pt5 + cn * 0.2); } return col; } vec3 batikMaskTex(vec2 uv, float cn) { vec3 colors[2] = vec3[2]( vec3(1.0, 0.019, 0.062), vec3(0.847, 0.847, 0.847) ); float pt6 = pattern(uv, 4.0, 12.0); // Base Pattern vec3 col = mix(colors[0], colors[1], pt6 + cn * 0.5); return col; } void mainImage( out vec4 fragColor, in vec2 fragCoord ) { vec2 uv = fragCoord/iResolution.xy; vec2 uvn = (fragCoord.xy - iResolution.xy * 0.5) / iResolution.y; float cn = fbm(uvn * 16.0); // Fixed noise for Rough Texture uv *= rmatrix(sin(iTime / 16.0) * 0.5); uv = abs(fract(uv * 2.0) - 0.5); float tf = trigFlower(uvn * rmatrix(-iTime / 16.0), 5.0); vec3 bgt = (tf \u0026lt; 0.9) ? batikBgTex(uv, cn) : batikMaskTex(uv, cn); // Output to screen fragColor = vec4(vec3(bgt),1.0); } Click to expand and view more ","title":"Making Batik in GLSL"},{"link":"/posts/voidpix-generating-pixelated-skybox/","text":" VoidPix is an asset pack based on generated procedural skyboxes in 1K resolution. It includes 22 unique procedurally generated skyboxes. No AI is involved, purely random sampling and generation technique.\nYou can find it on itch.\nVoidPix Skybox Pack. ","title":"VoidPix: Generating Pixelated Skybox"},{"link":"/posts/winning-micro-jam-void/","text":" Devoid Preview Introduction Micro Jam is a competitive 48-hour game jam for developers of all levels, from beginners to veterans. Held bi-weekly, it features a prize pool of over $110, sponsored by Mini Script. For Jam 33, the theme was “Void”, with the prerequisite “Death is not the end.”\nAfter brainstorming a few concepts, I decided to interpret both literally: a game where death itself drives progress. My goal was to fully embrace the theme and win Best Use of Theme.\nIdeating Competing solo, I latched onto an idea: what if progress only came through death? Players would need to die repeatedly to achieve higher scores, but as the game advanced, dying would become increasingly difficult.\nSince the theme was “Void,” I set the stage in outer space. Players would take on the role of a spaceman drawn toward a Spiraling Void, each death pulling them into a higher dimension of existence. The core loop became clear: reach the void, ascend, and chase a high score of ascension.\nGame Design With space as the backdrop, I fleshed out the narrative. The spaceman became a Void-Traveling Entity, ascending through dimensions to gain power.\nOrbiting the void were scattered pieces of space debris, acting as platforms to latch onto and launch from. Traversal required precision: misjudge a jump, and you’d drift into the emptiness of space.\nDevoid Divination Example To add depth, I designed power-ups with both buffs and debuffs. I enjoy creating upgrades where advantages come at a cost—too many buffs stack into a weakness. This duality forces players to strategize, ensuring every choice shapes their playstyle.\nPrototyping I built the game in Unity for Web. Core mechanics started with a flicking system—similar to Angry Birds—to propel the entity from one point to another.\nDevoid Debri Sprite Sampled (https://aikijeet.wordpress.com/2010/10/15/2010-gallery/debris-compilation/ ) For the debris, I sampled space-themed sprites and used my Palette Conversion Tool to map them into a cohesive color scheme. Levels were designed around a central void, with debris scattered outward in a spiral, implicitly guiding players toward the goal. Each level spawned the player at a greater radius, increasing difficulty over time.\nUpon reaching the void, players chose between buffs and debuffs. Some improved time or vision, while others restricted direction or reduced clarity. This interplay created a dynamic system where players tailored their own approach.\nPost Jam The game was well-received, especially for its creative use of the theme. Players found the gameplay challenging yet approachable, with a quirky premise that resonated with fellow developers.\nIn the end, I was thrilled to win first place overall at Micro Jam 33. It was a rewarding experience surrounded by talented developers and inventive games.\nYou can check out the game on Itch:\nDevoid: Ascend to the Void ","title":"Winning Micro Jam: Void"},{"link":"/posts/particle-simulation-playground-in-opengl/","text":"Introduction This project is a GPU-accelerated particle simulation built with OpenGL. Motion is calculated using vertex and compute shaders, where each particle follows defined rules of interaction and evolution.\nThe playground allows experimenting with different particle behaviors, from simple spawning to complex N-body simulations and mesh surface interactions.\nFeatures Spawning Particle Program\nParticles spawn with random lifetime, velocity, and color within a defined area.\nSingle Attractor\nParticles are drawn toward a movable attractor point on the screen.\nMulti Attractor\nParticles respond to multiple attractor points, each influencing their trajectory.\nN-Body Simulation\nParticles interact with one another using an inverse-square law, simulating gravitational-like forces.\nMesh Surface Estimation\nParticles are guided toward random points on a mesh surface, creating a surface-filling effect.\nConclusion The Particle Simulation Playground highlights the versatility of GPU programming with OpenGL. By leveraging shaders for motion, interaction, and rendering, it creates visually rich simulations that can scale efficiently while offering diverse particle behaviors.\n","title":"Particle Simulation Playground in OpenGL"},{"link":"/posts/sphere-ray-tracing-in-opengl/","text":" Preview Introduction This project demonstrates ray tracing and particle simulation using OpenGL. The scene combines static models, dynamic lights, and particle effects to showcase soft shadows, ambient occlusion, and real-time simulation.\nApproach The scene is built using a hybrid pipeline:\nRay Tracing\nStatic snowman model and dynamic light spheres are ray-traced. Soft shadows are computed using adjustable sample count and light radius. Spherical Ambient Occlusion enhances depth perception and realism. Early exit optimization is applied to rays with low attenuation for better performance. Particle Simulation\nParticles are rasterized while their motion is computed in the vertex shader. Dissolving and decay effects are implemented in geometry and fragment shaders. Simulation parameters such as particle count and size are configurable at runtime. Combination\nRay-traced scene and particle simulation are combined for the final output. Adjustable effects allow balancing visual quality and performance. Performance Tested on: Intel Core i5-1335U, Intel Iris Xe Graphics, 16GB RAM\nScene FPS (CPU–GPU) Ray Tracing Basic* 14–16 w/ Max Particle Simulation 13–14 w/ Max Shadow Sample 2–3 w/ Max Ambient Occlusion 2–2.5 *Basic configuration: 3–100 reflections, 16 shadow samples, 16 ambient occlusion samples, 4096 particles\nConclusion This project demonstrates a real-time hybrid rendering approach, combining ray tracing for realistic lighting and shadows with GPU-accelerated particle simulations. Optimizations like early exit and adjustable sampling make it feasible on mid-range hardware while maintaining high visual fidelity.\n","title":"Sphere Ray Tracing in OpenGL"},{"link":"/posts/mirror-using-offscreen-rendering-and-stencil-mask-in-opengl/","text":" Preview Introduction This project demonstrates how to create a mirror effect in OpenGL using stencil buffers, framebuffers, and object masking. The result is a composed scene where the mirror view is rendered realistically, with optional blur outside masked objects.\nApproach The mirror was built using a multi-pass rendering pipeline:\nMirror Camera Transformation\nThe mirror camera is derived from the main camera by reflecting its view matrix across the mirror plane.\n$$ V_{mirror} = V \\cdot M \\cdot R_z \\cdot M^{-1} $$\n$V$ = Main camera view matrix $M$ = Mirror transform $R_z$ = Reflection matrix across the z-axis Framebuffer Setup\nReal FBO: Renders the normal world view.\nMirror FBO: Renders the mirrored scene for use inside the mirror.\nMask FBO: Defines regions where blur should not be applied.\nScene Composition\nA shader composites the textures into a final full-screen quad:\nMirror content is blurred with Gaussian blur. Objects within the mask remain sharp. Performance Tested on: Intel Core i5-1335U, Intel Iris Xe Graphics, 16GB RAM\nScene FPS (CPU–GPU) Composed Scene 90–250 Conclusion This project shows how reflection matrices, FBOs, and masking can work together in OpenGL to create a performant, realistic mirror system. The method is modular and can be extended to portals, water reflections, or multi-layered effects.\nYou can find the project below.\nMirrorScene ","title":"Mirror using Offscreen Rendering and Stencil Mask in OpenGL"},{"link":"/posts/making-a-fast-and-featureful-color-changing-tool/","text":" Paleta Preview Paleta is a powerful Color Palette Extraction and Management Tool designed to enhance visual game assets. It allows designers \u0026amp; developers to extract, convert, mix, minimize, and match color palettes effortlessly.\nColors are treated as vectors (RGBA), while palettes are sets of colors supporting union, intersection, addition, and removal. Paleta integrates with Lospec for importing palettes and provides functions for mapping between palettes using distance metrics or custom algorithms. Users can also export palettes or map them onto images.\nWith tools to minimize, maximize, and randomize palettes, Paleta simplifies visual asset creation and ensures consistent, vibrant color management for any project.\nYou can find it on Pip.\nPython Package ","title":"Making a Fast and Featureful Color Changing Tool"}],"tags":[{"link":"/tags/-pixel-art/","name":" Pixel Art","slug":" Pixel Art"},{"link":"/tags/animation/","name":"Animation","slug":"Animation"},{"link":"/tags/c%23-programming/","name":"C# Programming","slug":"C# Programming"},{"link":"/tags/c++-programming/","name":"C++ Programming","slug":"C++ Programming"},{"link":"/tags/game-design/","name":"Game Design","slug":"Game Design"},{"link":"/tags/game-development/","name":"Game Development","slug":"Game Development"},{"link":"/tags/game-mechanics/","name":"Game Mechanics","slug":"Game Mechanics"},{"link":"/tags/glsl-programming/","name":"GLSL Programming","slug":"GLSL Programming"},{"link":"/tags/hlsl-programming/","name":"HLSL Programming","slug":"HLSL Programming"},{"link":"/tags/lut/","name":"LUT","slug":"LUT"},{"link":"/tags/opengl/","name":"OpenGL","slug":"OpenGL"},{"link":"/tags/particles/","name":"Particles","slug":"Particles"},{"link":"/tags/python/","name":"Python","slug":"Python"},{"link":"/tags/rendering/","name":"Rendering","slug":"Rendering"},{"link":"/tags/shaders/","name":"Shaders","slug":"Shaders"},{"link":"/tags/shapefile/","name":"Shapefile","slug":"Shapefile"},{"link":"/tags/skybox/","name":"Skybox","slug":"Skybox"},{"link":"/tags/sprite/","name":"Sprite","slug":"Sprite"},{"link":"/tags/ui/","name":"UI","slug":"UI"},{"link":"/tags/unity/","name":"Unity","slug":"Unity"}]}